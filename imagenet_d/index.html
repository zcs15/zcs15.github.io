<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ImageNet-D.">
  <meta name="keywords" content="Diffusion models, robustness evaluation, vision-language models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object</h1>
          <h3 class="title is-3 publication-title", style="color: red;"> CVPR 2024 Highlight (2.8%)</h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://chenshuang-zhang.github.io/">Chenshuang Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.feipan.info/">Fei Pan</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=GdQtWNQAAAAJ">Junmo Kim</a><sup>*1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=XA8EOlEAAAAJ">In So Kweon</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.columbia.edu/~mcz/">Chengzhi Mao</a><sup>*3,4</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>KAIST</span>
            <span class="author-block"><sup>2</sup>University of Michigan</span>
            <span class="author-block"><sup>3</sup> McGill University</span>
            <span class="author-block"><sup>4</sup>MILA</span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>Corresponding author</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2403.18775"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/chenshuang-zhang/imagenet_d"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/11zTXmg5yNjZwi8bwc541M1h5tPAVGeQc/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
                </span>

            <!-- Youtube Link. -->
            <span class="link-block">
            <a href="https://www.youtube.com/watch?v=CQm2oDfCvR8"
                class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <i class="fab fa-youtube"></i>
              </span>
              <span>Youtube</span>
              </a>
            </span>            
          </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="Citation">
  <div class="container is-max-desktop content">
    <!-- <h3 class="title">Citation</h3> -->
    <pre><code>@InProceedings{Zhang_2024_CVPR,
      author    = {Zhang, Chenshuang and Pan, Fei and Kim, Junmo and Kweon, In So and Mao, Chengzhi},
      title     = {ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object},
      booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
      month     = {June},
      year      = {2024},
      pages     = {21752-21762}
  }</code></pre>
  </div>
</section>

<section class="section" id="Teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <img src="static/images/teaser_figure.png" alt="Teaser Figure" />
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="Abstract">
  <div class="container is-max-desktop">
    <h3 class="title is-3 has-text-centered">Abstract</h3>
    <div class="content has-text-justified">
      <p>
        We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific types of evaluation over synthetic corruptions, backgrounds, and textures. Yet, those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative models as a data source for synthesizing hard images that benchmark deep models' robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, which we term as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60%. Our work suggests that diffusion models can be an effective source to test vision models.
      </p>
    </div>
  </div>
</section>

<section class="section" id="Method">
  <div class="container is-max-desktop">
    <h3 class="title is-3 has-text-centered">Method: Benchmark Robustness by Diffusion</h3>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <div class="content" id='mira_image'>
          <img src="static/images/framework.png" width="800px" height="2100px" />
        </div>
        <div class="content has-text-justified">
          <p>
            We create ImageNet-D by first generating a large image pool using diffusion models. To make the test set challenging, we only keep the hard images from the large pool that make multiple surrogate models fail. The test set is then refined through human verification to ensure the labeling quality.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="Results">
  <div class="container is-max-desktop">
    <h3 class="title is-3 has-text-centered">Results: SOTA Model Accuracy Drops by up to 60%</h3>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <div class="content" id='mira_image'>
          <img src="static/images/imagenet.png" width="800px" height="2100px" />
        </div>
        <div class="content has-text-justified">
          <p>
            Model accuracy on ImageNet vs. ImageNet-D. Each data point corresponds to one tested model. The plots reveal that there is a significant accuracy drop from ImageNet to our new test set ImageNet-D.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="Comparison">
  <div class="container is-max-desktop">
    <h3 class="title is-3 has-text-centered">Visualizations: Higher Quality and Diverse Variations</h3>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <div class="content" id='mira_image'>
          <img src="static/images/test_set_comparison.png" width="800px" height="2100px" />
        </div>
        <div class="content has-text-justified">
          <p>
            Compared to prior synthetic test sets, ImageNet-D achieves higher image quality and diverse variations. ImageNet-D can be scaled efficiently to include more categories and nuisances.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="DatasetSamples">
  <div class="container is-max-desktop">
    <h3 class="title is-3 has-text-centered">Visualizations: CLIP Fails on ImageNet-D</h3>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <div class="content" id='mira_image'>
          <img src="static/images/imagenet_d_samples.png" width="800px" height="2100px" />
        </div>
        <div class="content has-text-justified">
          <p>
            For each group of images, the ground truth label is color green, while the predicted categories by CLIP (ViT-L/14) on each image are in black.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="DatasetSamplesFailure">
  <div class="container is-max-desktop">
    <h3 class="title is-3 has-text-centered">Visualizations: MiniGPT-4 and LLaVa-1.5 Fail</h3>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <div class="content" id='mira_image'>
          <img src="static/images/minigpt_llava1_5.png" width="800px" height="2100px" />
        </div>
        <div class="content has-text-justified">
          <p>
            Both MiniGPT-4 and LLaVa-1.5 fail to recognize the object on ImageNet-D, highlighting the challenges the dataset poses for these models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
          Thanks to <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> for the source code of the nice website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
